{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# hey there! welcome to this interactive cuda notebook\n",
        "\n",
        "# this is meant to be a hands-on companion to the article at https://siboehm.com/articles/22/CUDA-MMM\n",
        "# if you haven't read it yet, thats fine! its probably good to read along with the exercises though\n",
        "\n",
        "# we're gonna dive into some cuda optimizations, starting with a basic implementation and working our way up to some pretty cool tricks.\n",
        "# you'll get to write actual cuda code and see how different techniques can boost performance\n",
        "\n",
        "# here's what you should probably know:\n",
        "\n",
        "# prerequisites:\n",
        "# - basic c++ (writing a for loop)\n",
        "# - basic python\n",
        "# - a bit of linear algebra (knowing what a matrix multiplication is)\n",
        "# - curiosity about making gpus go fast\n",
        "\n",
        "# feel free to google/read the article/ask a language model as you go if you get stuck\n",
        "# or provide the author of this notebook feedback on sections that confused you (ideal)\n",
        "\n",
        "# lets get some setup out of the way"
      ],
      "metadata": {
        "id": "dJPEWeC84Ybx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "3CIjdKCr4h3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version # make sure this works! this code will not work if you are using a CPU (you can use a T4 in colab)"
      ],
      "metadata": {
        "id": "Ui8P1SL-Lo6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell for setup\n",
        "\n",
        "# clones repo to get the other files\n",
        "!git clone https://github.com/arb8020/optimizing-matmul-exercises\n",
        "%cd optimizing-matmul-exercises\n",
        "\n",
        "# pip\n",
        "!pip install requests numpy\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "src_path = os.path.abspath('src')\n",
        "if src_path not in sys.path:\n",
        "    sys.path.append(src_path)\n",
        "\n",
        "from helper_functions import check_solution, compile_and_run_kernel\n",
        "\n",
        "!cp src/test_sgemm.cu .\n",
        "!cp src/test_sgemm2.cu .\n",
        "!cp src/test_sgemm3.cu .\n",
        "\n",
        "!nvcc src/test_sgemm.cu -c -o test_sgemm.o\n",
        "!nvcc src/test_sgemm2.cu -c -o test_sgemm2.o\n",
        "!nvcc src/test_sgemm3.cu -c -o test_sgemm3.o"
      ],
      "metadata": {
        "id": "CEynGoCNH_QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel 1: Naive Implementation"
      ],
      "metadata": {
        "id": "sIschUVxUUVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction"
      ],
      "metadata": {
        "id": "OkwIBc-3b1-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" CUDA Hierarchy \"\"\"\n",
        "\n",
        "# a CUDA kernel is an operation to be run on the GPU\n",
        "# Computation in CUDA is split into three levels: grid, block, thread\n",
        "# creating a kernel creates a grid, made up of blocks\n",
        "# each of these blocks has a bunch of threads which share memory\n",
        "\n",
        "# gridDim contains the amount of blocks in a grid\n",
        "# gridDim is a 3 size vector\n",
        "# gridDim.x, gridDim.y, gridDim.z\n",
        "\n",
        "# blockDim contains the amount of threads in the block\n",
        "# blockDim is a 3 size vector\n",
        "# blockDim.x, blockDim.y, blockDim.z\n",
        "# blocks can have up to 1024 threads"
      ],
      "metadata": {
        "id": "3h6Vz0_JbwIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"SGEMM\"\"\"\n",
        "\n",
        "# single precision\n",
        "# generalized\n",
        "# matrix multiplication\n",
        "\n",
        "# basic matrix multiplication\n",
        "# C = AB\n",
        "# generalized version allows us to easily linearly transform this\n",
        "# C = alpha * (AB) + beta * (C)\n",
        "# note that if alpha = 1, beta = 0, it simplifies to the basic form\n",
        "\n",
        "# A is of dimension M x K\n",
        "# B is of dimension K x N\n",
        "# producing matrix C of dimension M x N\n",
        "# each entry of C is the dot product of row in A and col in B\n",
        "\n",
        "# so if we iterate through K, i = 0 ... K\n",
        "# we compute the row for A as row * K + i\n",
        "# and compute the column for B as i * N + col\n",
        "\n",
        "# finally, we scale each index of C by alpha and beta"
      ],
      "metadata": {
        "id": "sMqBQpnjg32s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Naive Kernel Implementation \"\"\"\n",
        "# CUDA kernels are written from the perspective of a single thread\n",
        "# for example, if we write:\n",
        "# b[i] = a[i] + 10\n",
        "# it will do this function for all threads i\n",
        "\n",
        "# for our first kernel, we will let each thread compute an entry\n",
        "# to compute the right value, each thread needs to get the right index value\n",
        "# since we're in 2D, each thread needs to know:\n",
        "\n",
        "# which x and y block of threads its in\n",
        "# how big the blocks of threads are (x and y dims)\n",
        "# thread index inside the block (x and y axis)\n",
        "\n",
        "# say we have a matrix of size 5 x 5\n",
        "# a block size of 3 x 3 (9 threads per block)\n",
        "# and a 3 x 3 grid (could load matrix size 9 x 9)\n",
        "# it looks something like:\n",
        "# grid: [block_0], [block_1], ...\n",
        "# block: [thread_0, thread_1, ...]\n",
        "\n",
        "# example:\n",
        "# x: [0, 1, 2], [3, 4, 5], [6, 7, 8]\n",
        "# y: [0, 1, 2], [3, 4, 5], [6, 7, 8]\n",
        "\n",
        "# so the thread (3,7) would be\n",
        "\n",
        "# x = 3 -> thread_0 in block 1\n",
        "# y = 7 -> thread_1 in block 2\n",
        "\n",
        "# note that since the matrix is of size 5 x 5 in this example\n",
        "# we would need to check if the current thread is even used\n",
        "# in this example, y = 7 > 4 so we don't actually use that thread"
      ],
      "metadata": {
        "id": "YRqwvO6nfG5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "TTNpuf2Rb0I8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# .cu doesnt have syntax highlighting so we write it as a .cpp and then just change name later"
      ],
      "metadata": {
        "id": "GPuV8wTZ2rv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile kernel_1.cpp\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "#include \"test_sgemm.cu\"\n",
        "\n",
        "// CUDA kernel function for performing a naive implementation of GEMM (sgemm)\n",
        "// STUDENT\n",
        "__global__ void sgemm_naive(int M, int N, int K, float alpha, const float *A, const float *B, float beta, float *C) {\n",
        "    // think about the CUDA hierarchy\n",
        "\n",
        "\n",
        "    // calculate the x-coordinate of the current thread within the grid\n",
        "    // we will need to add threadIdx to some block offset\n",
        "    // we can probably use blockIdx, blockDim to determine the offset\n",
        "    uint x_block_offset = ...\n",
        "    const uint x = ...\n",
        "    // calculate the y-coordinate of the current thread within the grid\n",
        "    uint y_block_offset = ...\n",
        "    const uint y = ...\n",
        "\n",
        "    // check if the current thread's coordinates are within the bounds of the output matrix C\n",
        "    // this is known as a 'guard'\n",
        "    // we'll likely need to compare the above variables with the respective matrix dimensions\n",
        "    // TODO: fill out the if statement\n",
        "    if (condition) {\n",
        "        // initialize a temporary variable to store the dot product\n",
        "        float tmp = 0.0;\n",
        "\n",
        "        // compute the dot product of the x-th row of matrix A and the y-th column of matrix B\n",
        "        // TODO: add the correct stop/increment condition for the for loop\n",
        "        for (int i = 0; cond; ++1) {\n",
        "            // TODO: accumulate results of matrix dot product into tmp\n",
        "            // remember the dot product formula!\n",
        "            // A: current row * A_columns + loop counter\n",
        "            // B: loop counter * B_columns + current column\n",
        "            tmp += ...\n",
        "        }\n",
        "\n",
        "        // compute the final value for the element at position (x, y) in the output matrix C\n",
        "        // by multiplying the temporary result 'tmp' by 'alpha', adding the product of 'beta'\n",
        "        // and the existing value at that position in C, and storing the result in C\n",
        "\n",
        "        // TODO: compute the final value as described above\n",
        "        // make sure to write it into the correct index as well!\n",
        "        C[] = ...\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int M = 512, N = 512, K = 512; // MNK will always be equal\n",
        "\n",
        "    std::cout << \"testing naive SGEMM: \" << std::endl;\n",
        "\n",
        "    dim3 gridDim(CEIL_DIV(M, 32), CEIL_DIV(N, 32), 1);\n",
        "    dim3 blockDim(32, 32, 1);\n",
        "\n",
        "    test_sgemm(M, N, K, gridDim, blockDim, sgemm_naive);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "m5csY0FHBKzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this block will compile and run your code!\n",
        "!cp kernel_1.cpp kernel_1.cu\n",
        "!nvcc kernel_1.cu -o kernel1 -lcublas\n",
        "!./kernel1"
      ],
      "metadata": {
        "id": "f24qFKoTKNak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this block will check your solution\n",
        "with open('kernel_1.cu', 'r') as file:\n",
        "    user_kernel_1 = file.read()\n",
        "check_solution(1, user_kernel_1, M=512, N=512, K=512)"
      ],
      "metadata": {
        "id": "Ndibgj8i2zvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Roofline Model:"
      ],
      "metadata": {
        "id": "8oPrDrWq7QyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction"
      ],
      "metadata": {
        "id": "hQIEhTjU7Uh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Motivating Improvements: Roofline Model \"\"\"\n",
        "# the underlying concept that motivates the improvements in this article is the roofline model\n",
        "# roofline model: https://en.wikipedia.org/wiki/Roofline_model\n",
        "# the Roofline model helps us determine performance bottlenecks\n",
        "# on the X axis we have Arithmetic Intensity:\n",
        "# Arithmetic Intensity is the ratio of floating point operations to the amount of data transferred\n",
        "# FLOP/byte\n",
        "# on the Y axis we have Memory Bandwidth:\n",
        "# the rate of data transfer between memory and processor\n",
        "# Byte/s\n",
        "# our performance is FLOP/s - which we can plot as a function of our MB and AI\n",
        "# if we have high AI, we are doing a lot of computations per unit of data: compute bound\n",
        "# if we have high MB, we are getting data as fast as possible, likely from registers or L1 cache vs main memory\n",
        "# so the goal is to move towards higher AI by optimizing our operations per load\n",
        "# and move towards peak memory bandwidth by having as much data as close to the processor as possible\n",
        "\n",
        "# in the below code, we'll explore the theoretical upper bound that we're shooting for with our kernel"
      ],
      "metadata": {
        "id": "CVeBegwg7Ssr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "H8fMwQf07Z2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_roofline(m, n, k, memory_bandwidth, computational_performance, data_type_size):\n",
        "\n",
        "    operations = None  # calculate number of operations\n",
        "    loads = None  # calculate number of loads\n",
        "    stores = None  # calculate number of stores\n",
        "\n",
        "    data_transfer = None # total amount of data to transfer\n",
        "\n",
        "    computation_time = None  # time to perform operations\n",
        "\n",
        "    memory_bandwidth_bytes = None  # convert to bytes/second\n",
        "    data_transfer_time = None  # time to transfer data\n",
        "\n",
        "    # in the lower bound, we can imagine that GPUs are able to start processing data as soon as part of it loads in\n",
        "    # so the latency is only bounded by either compute time or data transfer time - whichever is larger\n",
        "    lower_bound_latency = None\n",
        "\n",
        "    # calculate performance (operations/time)\n",
        "    # make sure to convert back to gigaflops\n",
        "    gflops = None\n",
        "\n",
        "    return {\n",
        "        'operations': operations,\n",
        "        'data_transfer': data_transfer,\n",
        "        'computation_time': computation_time,\n",
        "        'data_transfer_time': data_transfer_time,\n",
        "        'lower_bound_latency': lower_bound_latency,\n",
        "        'gflops': gflops\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "# NVIDIA T4 GPU specifications\n",
        "# https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-t4/t4-tensor-core-datasheet-951643.pdf\n",
        "# note that we are using fp16\n",
        "memory_bandwidth = None  # do this in GB/s\n",
        "computational_performance = None  # TFLOPs\n",
        "data_type_size = None  # bytes (for float16)\n",
        "\n",
        "# test your implementation with different matrix dimensions\n",
        "# what upper bound do you find, and why?\n",
        "m, n, k = 512, 512, 512\n",
        "\n",
        "result = calculate_roofline(m, n, k, memory_bandwidth, computational_performance, data_type_size)\n",
        "\n",
        "print(\"Roofline Model Results:\")\n",
        "print(f\"Total operations: {result['operations']:,}\")\n",
        "print(f\"Computation time: {result['computation_time']:.6f} seconds\")\n",
        "print(f\"Data transfer time: {result['data_transfer_time']:.6f} seconds\")\n",
        "print(f\"Lower bound latency: {result['lower_bound_latency']:.6f} seconds\")\n",
        "print(f\"Performance: {result['gflops']:.2f} GFLOPS\")\n",
        "\n",
        "# check your solution\n",
        "from helper_functions import check_roofline_calculation\n",
        "\n",
        "check_roofline_calculation(calculate_roofline)\n",
        "\n",
        "# experiment with different matrix sizes or hardware specifications!\n",
        "# try changing the values of m, n, k, memory_bandwidth, or computational_performance\n",
        "# and run the calculation again to see how it affects the results"
      ],
      "metadata": {
        "id": "ZaM5jSZi5pqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### visuals"
      ],
      "metadata": {
        "id": "22hUzDnpBVIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here, we can visualize the roofline model and confirm the calculations we made before\n",
        "# we may not be able to hit the peak value, but we can certainly try!"
      ],
      "metadata": {
        "id": "g5vbgw4rB764"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# matrix dimensions\n",
        "m_values = [2**x for x in range(12)]  # test different values of m\n",
        "n = 512  # constant value for n\n",
        "k = 1024  # constant value for k\n",
        "\n",
        "# NVIDIA T4 GPU specifications\n",
        "memory_bandwidth = 320  # GB/s\n",
        "computational_performance = 65  # TFLOPs\n",
        "\n",
        "# data type\n",
        "data_type = np.float16\n",
        "data_type_size = np.dtype(data_type).itemsize  # size of the data type in bytes\n",
        "\n",
        "# lists to store the results\n",
        "arithmetic_intensity_values = []\n",
        "gflops_values = []\n",
        "\n",
        "for m in m_values:\n",
        "    # calculate the total number of operations\n",
        "    operations = 2 * m * k * n\n",
        "\n",
        "    # calculate the total amount of data transferred\n",
        "    data_transfer = (m * k + k * n + m * n) * data_type_size\n",
        "\n",
        "    # calculate arithmetic intensity (operations per byte)\n",
        "    arithmetic_intensity = operations / data_transfer\n",
        "    arithmetic_intensity_values.append(arithmetic_intensity)\n",
        "\n",
        "    # calculate the computation time\n",
        "    computation_time = operations / (computational_performance * 1e12)\n",
        "\n",
        "    # calculate the data transfer time\n",
        "    memory_bandwidth_bytes = memory_bandwidth * 1e9  # Convert to bytes/second\n",
        "    data_transfer_time = data_transfer / memory_bandwidth_bytes\n",
        "\n",
        "    # lower bound on latency: from latency hiding\n",
        "    lower_bound_latency = max(computation_time, data_transfer_time)\n",
        "\n",
        "    # calculate throughput\n",
        "    gflops = operations / (lower_bound_latency * 1e9)\n",
        "    gflops_values.append(gflops)\n",
        "\n",
        "# plot the results\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(arithmetic_intensity_values, gflops_values, marker='o', label='Theoretical GFLOPS')\n",
        "\n",
        "# plot memory bandwidth limit\n",
        "ai_values = np.linspace(min(arithmetic_intensity_values), max(arithmetic_intensity_values), 100)\n",
        "bw_limit = memory_bandwidth * ai_values\n",
        "plt.plot(ai_values, bw_limit, linestyle='--', label='Memory Bandwidth Limit')\n",
        "\n",
        "# Plot computational performance limit\n",
        "comp_limit = [computational_performance * 1e3] * len(ai_values)\n",
        "plt.plot(ai_values, comp_limit, linestyle='--', label='Computational Performance Limit')\n",
        "\n",
        "plt.xlabel('Arithmetic Intensity (FLOP/byte)')\n",
        "plt.ylabel('Throughput (GFLOPS)')\n",
        "plt.title('Roofline Model')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WU_ENoNvBTIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel 2: Global Memory Coalescing and Warps\n"
      ],
      "metadata": {
        "id": "I7f7lmgI_aeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction"
      ],
      "metadata": {
        "id": "tijuS2qJ6P7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Warps \"\"\"\n",
        "# to get better memory accesses: we need to talk about warps\n",
        "# warps are groups of 32 threads next to each other\n",
        "# these are the smallest unit of compute that get instructions together\n",
        "# they are instructed by a warp scheduler\n",
        "\n",
        "# warp schedulers control consecutive thread ids\n",
        "# so we might have\n",
        "# warp_0 : t_0 ... t_31\n",
        "# warp_1 : t_32 ... t_63\n",
        "\n",
        "# the overall consecutive thread id is computed as follows:\n",
        "# threadId = threadIdx.x + blockDim.x * (threadIdx.y + blockDim.y * threadIdx.z)\n",
        "\n",
        "# say our threads were accessing sequential memory\n",
        "# t_0 accesses address 0, t_1 gets address 1, etc\n",
        "# threads that are part of the same warp will get memory accesses all together\n",
        "# this is known as 'global memory coalescing'\n",
        "# note that the accesses can be out of order as long as the memory is contiguous\n",
        "# t_0 accessing 0, t_1 accessing 2, t_2 accessing 1 can get coalesced\n",
        "# this is kind of similar to cache line optimizations from CPU programming\n",
        "\n",
        "# we will try to take advantage of global memory coalescing below\n",
        "# by introducing a BLOCKSIZE variable"
      ],
      "metadata": {
        "id": "clFPZTjKkgaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "JRF9QPrjkb0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile kernel_2.cpp\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "#include \"test_sgemm.cu\"\n",
        "\n",
        "// added BLOCKSIZE as a constant\n",
        "// this will be 32: the amount of threads in a warp\n",
        "template <const uint BLOCKSIZE>\n",
        "__global__ void sgemm_coalesce(int M, int N, int K, float alpha, const float *A, const float *B, float beta, float *C) {\n",
        "\n",
        "\n",
        "    // change variable names to reflect we are trying to use rows/columns\n",
        "    // rather than generic x, y indices\n",
        "\n",
        "    // use the calculation for consecutive threadId to derive an index\n",
        "    // which takes advantage of global memory coalescing\n",
        "\n",
        "    // remember that the overall consecutive thread id is computed as follows:\n",
        "    // threadId = threadIdx.x + blockDim.x * (threadIdx.y + blockDim.y * threadIdx.z)\n",
        "    // also remember that we want to access memory within the same warp: threads 0-31\n",
        "\n",
        "    // the BLOCKSIZE constant may come in handy here\n",
        "    // further hints:\n",
        "    // we probably want to replace blockDim with BLOCKSIZE (# threads in a warp)\n",
        "    // for rows, we likely need to do division\n",
        "    // for columns we likely need to do modulo\n",
        "    const int cRow = ...\n",
        "    const int cCol = ...\n",
        "\n",
        "    // this is the same as the code from before\n",
        "    if (cRow < M && cCol < N) {\n",
        "      float tmp = 0.0;\n",
        "      for (int i = 0; i < K; ++i) {\n",
        "        tmp += A[cRow * K + i] * B[i * N + cCol];\n",
        "      }\n",
        "      C[cRow * N + cCol] = alpha * tmp + beta * C[cRow * N + cCol];\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int M = 512, N = 512, K = 512;\n",
        "\n",
        "    std::cout << \"testing SGEMM with Coalescing: \" << std::endl;\n",
        "\n",
        "    // make blockDim one dimensional\n",
        "    dim3 gridDim(CEIL_DIV(M, 32), CEIL_DIV(N, 32));\n",
        "    dim3 blockDim(32 * 32);\n",
        "\n",
        "    test_sgemm(M, N, K, gridDim, blockDim, sgemm_coalesce<32>);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "wAaep03_Mxbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kernel_2.cpp kernel_2.cu\n",
        "\n",
        "!nvcc kernel_1.cu -o kernel1 -lcublas\n",
        "!./kernel1\n",
        "!nvcc kernel_2.cu -o kernel_2 -lcublas\n",
        "!./kernel_2"
      ],
      "metadata": {
        "id": "qhIg2XuDV32m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gDfk8Ymt3T-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this block will check your solution\n",
        "with open('kernel_2.cu', 'r') as file:\n",
        "    user_kernel_2 = file.read()\n",
        "check_solution(2, user_kernel_2, M=512, N=512, K=512)"
      ],
      "metadata": {
        "id": "WNaCEDdX3Qtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel 3: Shared Memory\n"
      ],
      "metadata": {
        "id": "PXMSfwVkAE2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction"
      ],
      "metadata": {
        "id": "i1Mdrd98z8B_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Memory Hierarchy \"\"\"\n",
        "# Let's spell out the Memory Hierarchy of a GPU:\n",
        "# understanding this will help us increase our Memory Bandwidth\n",
        "# DRAM: very big, but very slow\n",
        "# L2 Cache: 1000x smaller, but way faster (not on chip)\n",
        "# SM: streaming multiprocessor: each block of threads gets executed by one\n",
        "# each SM gets SMEM, shared memory on chip (kinda like L1 cache), and registers\n",
        "\n",
        "# to optimize for Byte/s we want our code to use\n",
        "# registers > SMEM > L2 > DRAM\n",
        "\n",
        "# so rather than reading from L2/DRAM (global memory) every time\n",
        "# we can instead load chunks of A's row and B's column\n",
        "# we'll choose a chunk size that fits onto the SMEM\n",
        "# this will allow us to compute and add to C more efficiently\n",
        "# since the values we need will be in almost the closest possible access point\n",
        "\n",
        "# note that when we use shared memory we need to avoid race conditions\n",
        "# if some threads start running too far ahead of others\n",
        "# we might write to memory that threadA needed to read before threadB wrote to it\n",
        "# CUDA provides a function called __syncthreads()\n",
        "# __syncthreads() effectively stops all the threads at that line of code until the other threads get there\n",
        "# this is important especially when loading from shared memory\n",
        "# so that the threads won't start working until all other threads have finished loading into shared memory"
      ],
      "metadata": {
        "id": "cuxEHZlEz71b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "5zyDe0Jy6BrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile kernel_3.cpp\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "#include \"test_sgemm.cu\"\n",
        "\n",
        "template <const uint BLOCKSIZE>\n",
        "__global__ void sgemm_shared(int M, int N, int K, float alpha, const float *A, const float *B, float beta, float *C) {\n",
        "\n",
        "  // we basically want to break this index from above down into smaller parts\n",
        "  // so that we can load chunks into shared memory\n",
        "  // const int cRow = blockIdx.x * BLOCKSIZE + (threadIdx.x / BLOCKSIZE);\n",
        "  // const int cCol = blockIdx.y * BLOCKSIZE + (threadIdx.x % BLOCKSIZE);\n",
        "\n",
        "  // first get output block row and col (first part)\n",
        "  // TODO: first part of the index calculation\n",
        "  const uint cRow = blockIdx.x;\n",
        "  const uint cCol = blockIdx.y;\n",
        "\n",
        "  // get the inner row and column\n",
        "  // TODO: second part of the index calculation\n",
        "  const uint threadCol = threadIdx.x % BLOCKSIZE;\n",
        "  const uint threadRow = threadIdx.x / BLOCKSIZE;\n",
        "\n",
        "  // move pointers in A, B, C to the right spots\n",
        "  // we want A such that row = cRow, col = 0\n",
        "  // we want B such that row = 0, col = cCol\n",
        "  // we want C such that row = cRow, col = cCol\n",
        "\n",
        "  // we'll need to incorporate:\n",
        "  // blocksize (size of matrix we are going to load in)\n",
        "  // dimension of matrix A, B\n",
        "  A += ...\n",
        "  B += ...\n",
        "  C += ...\n",
        "\n",
        "  // allocate shared memory for the current block\n",
        "  // we would need the total size of the matrix: rows * columns\n",
        "  // in this case they should both be blocksize\n",
        "\n",
        "  __shared__ float As[BLOCKSIZE * BLOCKSIZE];\n",
        "  // TODO: write the shared memory allocation for B\n",
        "\n",
        "  // temporary float\n",
        "  float tmp = 0.0;\n",
        "\n",
        "  // here we iterate through the larger matrix, and actually load in the chunks\n",
        "  // we prob don't want blockIdx to be larger than the shared dimension\n",
        "  // and we probably should iterate in steps of blocksize\n",
        "  // TODO: fill in for loop\n",
        "  for (int blockIdx; blockIdx_comparison; blockIdx_increment) {\n",
        "\n",
        "    // load chunk into shared memory for A and B\n",
        "    // the index to load into is likely a function of\n",
        "    // threadRow, threadCol and BLOCKSIZE (since the shared memory space used blocksize)\n",
        "\n",
        "    // the index to load from is likely similar\n",
        "    // it should be a function of the desired row, number of columns, and desired column\n",
        "    As[] = A[];\n",
        "    Bs[] = B[];\n",
        "\n",
        "    // we don't want to move on until all the shared memory has been loaded\n",
        "    // how can we do that?\n",
        "    // TODO: prevent race conditions here\n",
        "\n",
        "    // TODO: compute dot product for the specified block\n",
        "    for (int dotIdx; dotIdx_comparison; dotIdx_increment) {\n",
        "      // TODO: fill in the indices\n",
        "      // this should look pretty similar to kernel 1\n",
        "      // A: row * multiplicative offset + index\n",
        "      // B: index * multiplicative offset + col\n",
        "      tmp += As[] * Bs[];\n",
        "    }\n",
        "\n",
        "    // TODO: move A, B pointers\n",
        "    // this probably uses the blocksize\n",
        "\n",
        "    A += ...\n",
        "    B += ...\n",
        "\n",
        "    // TODO: avoid fetching the next block into the cache before slower threads are done\n",
        "\n",
        "\n",
        "  }\n",
        "\n",
        "\n",
        "  // TODO: fill in the indices to correctly iterate through C\n",
        "  // and do the affine transformation\n",
        "  C[] = alpha * tmp + beta * C[];\n",
        "\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int M = 512, N = 512, K = 512;\n",
        "\n",
        "    std::cout << \"testing SGEMM with Shared Memory: \" << std::endl;\n",
        "\n",
        "    // make blockDim one dimensional\n",
        "    dim3 gridDim(CEIL_DIV(M, 32), CEIL_DIV(N, 32));\n",
        "    dim3 blockDim(32 * 32);\n",
        "\n",
        "    test_sgemm(M, N, K, gridDim, blockDim, sgemm_shared<32>);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "9Pz8fKz_zwAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kernel_3.cpp kernel_3.cu\n",
        "!nvcc kernel_3.cu -o kernel_3 -lcublas\n",
        "!./kernel_3"
      ],
      "metadata": {
        "id": "sCfQtO3PWEaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('kernel_3.cu', 'r') as file:\n",
        "    user_kernel_3 = file.read()\n",
        "check_solution(3, user_kernel_3, M=512, N=512, K=512)"
      ],
      "metadata": {
        "id": "egLtNiWe3xE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel 4: 1D Blocktiling for Calculating Multiple Results per Thread"
      ],
      "metadata": {
        "id": "8dix7lyJAMV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction"
      ],
      "metadata": {
        "id": "BrzBsb_26LFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Increasing Arithmetic Intensity \"\"\"\n",
        "# while we managed to move everything into shared memory\n",
        "# profiling would show that our calculations are stalling\n",
        "# this is because we are waiting on SMEM accesses\n",
        "# while SMEM accesses are fast, registers are still best\n",
        "# so we want our threads to use more register memory\n",
        "# right now each thread only computes 1 cell, but a register can hold more data than that\n",
        "\n",
        "# if we have threads use register memory more, we won't wait on SMEM accesses as much\n",
        "# to have threads use more register memory, each thread should compute more than just 1 cell\n",
        "\n",
        "# if you remember arithmetic intensity from the roofline model portion: this helps with that!\n",
        "# rather than calculating 1 result per thread\n",
        "# (loads 7 values from A and B, loads/stores 1 value into C -> 15 loads, 1 store per result)\n",
        "# we can compute 4 results per thread\n",
        "# (loads 14 values from A and B, 4 load/stores into C -> 8 loads, 1 store per result)\n",
        "\n",
        "# we will start by calculating a column of results at a time: 1D Blocktiling"
      ],
      "metadata": {
        "id": "q_-XjLgiKXgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "05GB8DoG6G3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile kernel_4.cpp\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "#include \"test_sgemm.cu\"\n",
        "\n",
        "// instead of using a flat blocksize\n",
        "// we define\n",
        "// BM: blocksize for dim M (A)\n",
        "// BN: blocksize for dim N (B)\n",
        "// BK: blocksize for dim K (A and B)\n",
        "// TM: thread mult factor (# entries calc per thread)\n",
        "\n",
        "template <const int BM, const int BN, const int BK, const int TM>\n",
        "__global__ void sgemm_blocktiling_1d(int M, int N, int K, float alpha, const float *A, const float *B, float beta, float *C) {\n",
        "\n",
        "  const uint cRow = blockIdx.x;\n",
        "  const uint cCol =blockIdx.y;\n",
        "\n",
        "  // each warp used to calculate 32 elements (1 per thread)\n",
        "  // now we want to calculate 32 * thread_mult_factor (# entries per thread)\n",
        "  // so we need to adjust threadCol, threadRow to account for this\n",
        "  // we'll need to access the blockSize for the relevant dimension\n",
        "  // TODO: fill in threadCol and threadRow\n",
        "  const int threadCol = ...\n",
        "  const int threadRow = ...\n",
        "\n",
        "  // allocate shared memory using blocksizes for A and B\n",
        "  // again, we will need to use the respective blocksizes\n",
        "  // TODO: fill it the correct size of array to allocate\n",
        "  __shared__ float As[];\n",
        "  __shared__ float Bs[];\n",
        "\n",
        "  // move pointers similarly to above\n",
        "  // TODO: use BM, BN, BK instead of the flat blocksize\n",
        "  A += ...\n",
        "  B += ...\n",
        "  C += ...\n",
        "\n",
        "  // we are making another inner loop\n",
        "  // so we will need to know where we are inside of A and B\n",
        "  // for warp-level GMEM coalescing\n",
        "  // TODO: determine the inner row/column we will need\n",
        "  const uint innerColA = ...\n",
        "  const uint innerRowA = ...\n",
        "  const uint innerColB = ...\n",
        "  const uint innerRowB = ...\n",
        "\n",
        "  // before we stored a scalar\n",
        "  // now since we are doing multiple entries we must store more values\n",
        "  float threadResults[TM] = {0.0};\n",
        "\n",
        "  // TODO: replace with the appropriate variable instead of BLOCKSIZE\n",
        "  for (int blockIdx = 0; blockIdx < K; blockIdx += BLOCKSIZE) {\n",
        "    // load chunks into shared memory as before\n",
        "    // remember to prevent against race conditions\n",
        "    // TODO: use innerRow/Col instead of threadRow/Col\n",
        "\n",
        "    As[] = A[];\n",
        "    Bs[] = B[];\n",
        "\n",
        "\n",
        "    // dot product again\n",
        "    // TODO: replace BLOCKSIZE\n",
        "    for (uint dotIdx = 0; dotIdx < BLOCKSIZE; ++dotIdx) {\n",
        "      // we can store the B value that is needed for all of our computations here\n",
        "\n",
        "      // TODO: fill out what tmpB initializes to: use the correct index\n",
        "      float tmpB = Bs[];\n",
        "\n",
        "      // remember that we are now doing multiple operations per thread\n",
        "      // we'll probably need TM (thread multiplication factor) somewhere\n",
        "      // TODO: fill in the for loop to add partial results from A\n",
        "      for (uint resultIdx = 0; resultIdxcomparison; resultIdxincrement) {\n",
        "        // how will we compute the correct index to use from As?\n",
        "        // we need to combine the logic of iterating multiple rows\n",
        "        // with the dotproduct logic\n",
        "\n",
        "        // TODO: use the correct index for As to compute the dot product for this column\n",
        "        threadResults[resultIdx] += As[] * tmpB;\n",
        "      }\n",
        "\n",
        "    }\n",
        "\n",
        "    // move A, B pointer to next tile\n",
        "    // TODO: replace BLOCKSIZE\n",
        "    A += BLOCKSIZE;\n",
        "    B += BLOCKSIZE * N;\n",
        "\n",
        "    // remember to syncthreads\n",
        "\n",
        "  }\n",
        "\n",
        "  // generalize for C again\n",
        "  // need to do this multiple times per thread now\n",
        "  for (uint resultIdx = 0; resultIdxcomparison; resultIdxincrement) {\n",
        "    // similar to the logic from As above\n",
        "    // we need to combine the usual logic with computing a column for each thread\n",
        "    // TODO: use the correct index\n",
        "    C[] = alpha * threadResults[resultIdx] + beta * C[];\n",
        "  }\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int M = 4096, N = 4096, K = 4096;\n",
        "\n",
        "    std::cout << \"testing SGEMM with 1D Blocktiling: \" << std::endl;\n",
        "\n",
        "    const uint BM = 64;\n",
        "    const uint BN = 64;\n",
        "    const uint BK = 8;\n",
        "    const uint TM = 8;\n",
        "\n",
        "    dim3 gridDim(CEIL_DIV(N, BN), CEIL_DIV(M, BM));\n",
        "    dim3 blockDim((BM * BN) / TM);\n",
        "\n",
        "    test_sgemm(M, N, K, gridDim, blockDim, sgemm_blocktiling_1d<BM, BN, BK, TM>);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "-cX1LxDL5-6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kernel_4.cpp kernel_4.cu\n",
        "!nvcc kernel_4.cu -o kernel_4 -lcublas\n",
        "!./kernel_4"
      ],
      "metadata": {
        "id": "_ETO69YiWYA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('kernel_4.cu', 'r') as file:\n",
        "    user_kernel_4 = file.read()\n",
        "check_solution(4, user_kernel_4, M=4096, N=4096, K=4096)"
      ],
      "metadata": {
        "id": "onuIeJ2MnB5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel 5: Increasing Arithmetic Intensity via 2D Blocktiling\n"
      ],
      "metadata": {
        "id": "MC6f9b5NAQq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction"
      ],
      "metadata": {
        "id": "F6fy_ExdDjsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here, we expand upon the previous kernel, moving to calculating a 2D tile per thread\n",
        "# rather than just a column, we compute a tile of rows and columns\n",
        "# this further increases our arithmetic intensity"
      ],
      "metadata": {
        "id": "WnKDK7WzlBnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "5w4LLWZGDjiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile kernel_5.cpp\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "#include <cassert>\n",
        "\n",
        "#include \"test_sgemm2.cu\"\n",
        "\n",
        "\n",
        "// we add TN: the amount of rows we want to compute per thread in addition to TM\n",
        "template <const int BM, const int BN, const int BK, const int TM, const int TN>\n",
        "__global__ void __launch_bounds__((BM * BN) / (TM * TN), 1) sgemm_blocktiling_2d(int M, int N, int K, float alpha, const float *A, const float *B, float beta, float *C) {\n",
        "\n",
        "  // as before\n",
        "  const uint cRow = blockIdx.x;\n",
        "  const uint cCol = blockIdx.y;\n",
        "\n",
        "  // our threads are now doing a whole 2D tile worth of work\n",
        "  const uint totalResultsBlocktile = BM * BN;\n",
        "\n",
        "  // threads per blocktile\n",
        "  // this is because we are\n",
        "  const uint numThreadsBlocktile = totalResultsBlocktile / (TM * TN);\n",
        "\n",
        "  // we need to adjust this number by the amount of threads we use\n",
        "  // TODO: use BN and TN to get the correct threadCol/threadRow\n",
        "  const int threadCol = ...\n",
        "  const int threadRow = ...\n",
        "\n",
        "\n",
        "  // allocate space as before\n",
        "  __shared__ float As[BM * BK];\n",
        "  __shared__ float Bs[BK * BN];\n",
        "\n",
        "  // move pointers as before\n",
        "  A += cRow * BM * K;\n",
        "  B += cCol * BN;\n",
        "  C += cRow * BM * N + cCol * BN;\n",
        "\n",
        "  // inner loop as before\n",
        "  const uint innerColA = threadIdx.x % BK;\n",
        "  const uint innerRowA = threadIdx.x / BK;\n",
        "  const uint innerColB = threadIdx.x % BN;\n",
        "  const uint innerRowB = threadIdx.x / BN;\n",
        "\n",
        "  // as before, we need to allocate cache for the grid of results\n",
        "  float threadResults[TM * TN] = {0.0};\n",
        "\n",
        "  // similar to tmpB from before\n",
        "  // we now need to allocate space to store temporary results in each thread\n",
        "  float regM[TM] = {0.0};\n",
        "  float regN[TN] = {0.0};\n",
        "\n",
        "  // loop over block tiles\n",
        "  for (uint blockIdx = 0; blockIdx < K; blockIdx += BK) {\n",
        "\n",
        "    // load chunks incrementing by stride, within the size BM/BN\n",
        "    // we now need to load chunks for each thread, taking into consideration both rows and columns\n",
        "\n",
        "    // these should technically be outside the loop but they make more logical sense here\n",
        "    // for implementation hints\n",
        "    const uint strideA = ...\n",
        "    const uint strideB = ...\n",
        "\n",
        "    // TODO: use strideA, strideB and BM/BN\n",
        "    // to figure out how to load a chunk of data from A/B for each thread\n",
        "    for (uint loadOffset; loadOffsetComparison; loadOffsetIncrement) {\n",
        "      // this should look pretty similar to how we loaded in\n",
        "      // recall As[innerRowA * BK + innerColA] = A[innerRowA * K + innerColA];\n",
        "      // since we are now loading a chunk of rows instead of just one\n",
        "      // we need to add a bit more math to this index\n",
        "      // TODO: determine the indices to load from/into\n",
        "      As[] = A[];\n",
        "    }\n",
        "    // TODO: do the same for B\n",
        "\n",
        "    // syncthreads\n",
        "    __syncthreads()\n",
        "\n",
        "    // advance blocktile as before\n",
        "    A += BK;\n",
        "    B += BK * N;\n",
        "\n",
        "    // modify inner loop for dot product to be 2d logic rather than 1d\n",
        "    // we're basically doing kernel 1 here for each thread\n",
        "    for (uint dotIdx = 0; dotIdxComparison; dotIdxIncrement) {\n",
        "      // TODO: load data into registers from A shared\n",
        "      for (uint i = 0; iComparison; iIncrement) {\n",
        "        // TODO: compute index to read from\n",
        "        // again, the indexing should look pretty familiar\n",
        "        regM[i] = As[];\n",
        "      }\n",
        "      // repeat logic for regN, Bs\n",
        "\n",
        "      // accumulate into threadResults\n",
        "      // TODO: expand the previous kernel from 1d to 2d - use the registers!\n",
        "      for (uint resIdxM; resIdxMComparison; ++resIdxM) {\n",
        "        for (uint resIdxN; resIdxNComparison; ++resIdxN) {\n",
        "          // TODO: compute index to accumulate into\n",
        "          // again should look p similar to logic found in kernel 1\n",
        "          threadResults[] += regM[resIdxM] * regN[resIdxN];\n",
        "        }\n",
        "      }\n",
        "\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "  }\n",
        "\n",
        "  // generalize to C\n",
        "  // still has to be done in the tile for each thread\n",
        "  for (uint resIdxM = 0; resIdxMComparison; ++resIdxM) {\n",
        "    for (uint resIdxN = 0; resIdxNComparison; ++resIdxN) {\n",
        "      // TODO: determine the correct index for correctly iterating through the whole matrix\n",
        "      // recall: C[(threadRow * TM + resultIdx) * N + threadCol]\n",
        "      // how do we incorporate resIdxM/N, TM/TN instead of just resultIdx from the 1d case?\n",
        "      C[] = alpha * threadResults[] + beta * C[];\n",
        "    }\n",
        "  }\n",
        "\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int M = 4096, N = 4096, K = 4096;\n",
        "\n",
        "    std::cout << \"testing SGEMM with 2D Blocktiling: \" << std::endl;\n",
        "\n",
        "    const uint BK = 8;\n",
        "    const uint TM = 8;\n",
        "    const uint TN = 8;\n",
        "    const uint BM = 128;\n",
        "    const uint BN = 128;\n",
        "    dim3 gridDim(CEIL_DIV(N, BN), CEIL_DIV(M, BM));\n",
        "    dim3 blockDim((BM * BN) / (TM * TN));\n",
        "    test_sgemm(M, N, K, gridDim, blockDim, sgemm_blocktiling_2d<BM, BN, BK, TM, TN>);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "Bnrk4umjDiVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kernel_5.cpp kernel_5.cu\n",
        "!nvcc kernel_5.cu -o kernel_5 -lcublas\n",
        "!./kernel_5"
      ],
      "metadata": {
        "id": "6W1Sxs7DWeIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('kernel_5.cu', 'r') as file:\n",
        "    user_kernel_5 = file.read()\n",
        "check_solution(5, user_kernel_5, M=4096, N=4096, K=4096)"
      ],
      "metadata": {
        "id": "2O2qdTBi4BWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel 6: Vectorize SMEM and GMEM Accesses\n"
      ],
      "metadata": {
        "id": "okf64Dx3ARPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction"
      ],
      "metadata": {
        "id": "hDQNYfMjUKno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we can still push for even more performance!\n",
        "# if we look at the assembly that our code outputs\n",
        "# we would notice that loading in data from Bs into regN is a vectorized operation\n",
        "# so with one instruction, we got multiple pieces of data loaded in\n",
        "# but this doesn't happen with As, since the memory isn't contiguous\n",
        "# if we can get As into the same shape as Bs, we should get the vectorized loads\n",
        "# so we just transpose A when we load it into As, and that should do the trick\n",
        "#\n",
        "# syntax tips here -> https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/"
      ],
      "metadata": {
        "id": "-d9q9SiqUNkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "qMJ03bdiUMFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile kernel_6.cpp\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "#include <cassert>\n",
        "\n",
        "#include \"test_sgemm3.cu\"\n",
        "\n",
        "\n",
        "template <const int BM, const int BN, const int BK, const int TM, const int TN>\n",
        "// we remove A and B from being const float *\n",
        "// to being float *\n",
        "// because we want to load w vector instructions\n",
        "__global__ void sgemm_vector(int M, int N, int K, float alpha, float *A, float *B, float beta, float *C) {\n",
        "\n",
        "  // as before\n",
        "  const uint cRow = blockIdx.x;\n",
        "  const uint cCol = blockIdx.y;\n",
        "\n",
        "\n",
        "  // as before\n",
        "  const int threadCol = threadIdx.x % (BN / TN);\n",
        "  const int threadRow = threadIdx.x / (BN / TN);\n",
        "\n",
        "  // allocate space as before\n",
        "  __shared__ float As[BM * BK];\n",
        "  __shared__ float Bs[BK * BN];\n",
        "\n",
        "  // move pointers as before\n",
        "  A += cRow * BM * K;\n",
        "  B += cCol * BN;\n",
        "  C += cRow * BM * N + cCol * BN;\n",
        "\n",
        "  // we will load 4 floats at a time\n",
        "  // this is because we will use the register float4 provided by cuda\n",
        "  // TODO: make sure we have 4 elements per thread\n",
        "  const uint innerRowA = ...\n",
        "  const uint innerColA = ...\n",
        "  const uint innerRowB = ...\n",
        "  const uint innerColB = ...\n",
        "\n",
        "  // as before\n",
        "  float threadResults[TM * TN] = {0.0};\n",
        "  float regM[TM] = {0.0};\n",
        "  float regN[TN] = {0.0};\n",
        "\n",
        "\n",
        "  // loop over block tiles as before\n",
        "  for (uint blockIdx = 0; blockIdx < K; blockIdx += BK) {\n",
        "    // reinterpret_cast allows us to use the vector data type\n",
        "\n",
        "    // TODO: load B using the vector instructions and float4\n",
        "\n",
        "    // TODO: transpose A while loading it into the vector data type float4\n",
        "\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // advance blocktile as before\n",
        "    A += BK;\n",
        "    B += BK * N;\n",
        "\n",
        "    // 2d blocktiling loop as before\n",
        "    for (uint dotIdx = 0; dotIdx < BK; ++dotIdx) {\n",
        "      // load into registers\n",
        "      // TODO: rewrite the logic for reading into regM\n",
        "      // remember how we transposed A when loading into As?\n",
        "      for (uint i = 0; i < TM; ++i) {\n",
        "        regM[i] = ...\n",
        "      }\n",
        "      for (uint i = 0; i < TN; ++i) {\n",
        "        regN[i] = Bs[dotIdx * BN + threadCol * TN + i];\n",
        "      }\n",
        "\n",
        "      // same as before\n",
        "      for (uint resIdxM = 0; resIdxM < TM; ++resIdxM) {\n",
        "        for (uint resIdxN = 0; resIdxN < TN; ++resIdxN) {\n",
        "          threadResults[resIdxM * TN + resIdxN] +=\n",
        "              regM[resIdxM] * regN[resIdxN];\n",
        "        }\n",
        "      }\n",
        "\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "  }\n",
        "\n",
        "  // generalize to C\n",
        "  // TODO: use vector instructions now\n",
        "\n",
        "  for (uint resIdxM = 0; resIdxM < TM; resIdxM += 1) {\n",
        "    // change the increment to abide by using float4\n",
        "    for (uint resIdxN = 0; resIdxN < TN; resIdxN_increment) {\n",
        "\n",
        "      // TODO: affine transform while loading into vector\n",
        "\n",
        "      // write tmp with vector instructions\n",
        "\n",
        "    }\n",
        "  }\n",
        "\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int M = 4096, N = 4096, K = 4096;\n",
        "\n",
        "    std::cout << \"testing SGEMM with 2D Blocktiling and Vectorizing: \" << std::endl;\n",
        "\n",
        "    const uint BK = 8;\n",
        "    const uint TM = 8;\n",
        "    const uint TN = 8;\n",
        "    const uint BM = 128;\n",
        "    const uint BN = 128;\n",
        "    dim3 gridDim(CEIL_DIV(N, BN), CEIL_DIV(M, BM));\n",
        "    dim3 blockDim((BM * BN) / (TM * TN));\n",
        "    test_sgemm(M, N, K, gridDim, blockDim, sgemm_vector<BM, BN, BK, TM, TN>);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "luUOo5IhOlrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kernel_6.cpp kernel_6.cu\n",
        "!nvcc kernel_6.cu -o kernel_6 -lcublas\n",
        "!./kernel_6"
      ],
      "metadata": {
        "id": "YFB7e3MAWpUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('kernel_6.cu', 'r') as file:\n",
        "    user_kernel_6 = file.read()\n",
        "check_solution(6, user_kernel_6, M=4096, N=4096, K=4096)"
      ],
      "metadata": {
        "id": "IiEd32q9gQY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H5tMvbv8Xpdd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
